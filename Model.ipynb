{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies (run in Colab)\n",
        "!pip install --upgrade pip\n",
        "!pip install timm pytesseract transformers sentence-transformers scikit-image python-magic\n",
        "!apt-get install -y tesseract-ocr || true"
      ],
      "metadata": {
        "id": "jQ5_owv9AZSE",
        "outputId": "6c21186a-2b61-4747-e68e-566c27e502f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "jQ5_owv9AZSE",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.2\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.20)\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (0.25.2)\n",
            "Collecting python-magic\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.23.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.35.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from pytesseract) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.1.10)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (3.5)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (2025.9.30)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.13.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Installing collected packages: python-magic, pytesseract\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [pytesseract]\n",
            "\u001b[1A\u001b[2KSuccessfully installed pytesseract-0.3.13 python-magic-0.4.27\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "41141c0f",
      "metadata": {
        "id": "41141c0f"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import json\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from PIL import Image, ImageFilter\n",
        "\n",
        "\n",
        "import timm\n",
        "from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "import pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_dataset_path = '/content/drive/MyDrive/DocuForge/dataset'\n",
        "local_dataset_path = '/content/dataset'\n",
        "\n",
        "# Function to copy dataset with progress\n",
        "def copy_dataset(src, dst):\n",
        "    if not os.path.exists(dst):\n",
        "        os.makedirs(dst)\n",
        "\n",
        "    for root, dirs, files in os.walk(src):\n",
        "        # Recreate directory structure\n",
        "        rel_path = os.path.relpath(root, src)\n",
        "        dest_dir = os.path.join(dst, rel_path)\n",
        "        os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "        # Copy files with progress bar\n",
        "        for file in tqdm(files, desc=f\"Copying {rel_path}\", unit=\"file\"):\n",
        "            src_file = os.path.join(root, file)\n",
        "            dest_file = os.path.join(dest_dir, file)\n",
        "            if not os.path.exists(dest_file):\n",
        "                shutil.copy2(src_file, dest_file)\n",
        "\n",
        "# Run it\n",
        "copy_dataset(drive_dataset_path, local_dataset_path)\n",
        "\n",
        "print(\"‚úÖ Dataset copied successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW4uHtK4ZDJ_",
        "outputId": "73a89986-1cf0-4fe1-950b-a35fffb97c36"
      },
      "id": "MW4uHtK4ZDJ_",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying .: 0file [00:00, ?file/s]\n",
            "Copying test: 0file [00:00, ?file/s]\n",
            "Copying test/authentic: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:06<00:00, 45.92file/s] \n",
            "Copying test/forged: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:08<00:00, 34.38file/s] \n",
            "Copying train: 0file [00:00, ?file/s]\n",
            "Copying train/forged: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1400/1400 [02:02<00:00, 11.46file/s]\n",
            "Copying train/authentic: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1400/1400 [01:28<00:00, 15.85file/s]\n",
            "Copying val: 0file [00:00, ?file/s]\n",
            "Copying val/authentic: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:05<00:00, 58.11file/s] \n",
            "Copying val/forged: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:11<00:00, 25.42file/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset copied successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/dataset\"\n",
        "\n",
        "# Configuration\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', DEVICE)\n",
        "\n",
        "# Paths (adjust for your Colab/Drive setup)\n",
        "DRIVE_DATA_PATH = '/content/drive/MyDrive/DocuForge/dataset' # if mounted\n",
        "LOCAL_DATA_PATH = '/content/dataset'\n",
        "\n",
        "IMG_SIZE = 384\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 4\n",
        "SAVE_DIR = 'saved_models_hybrid'\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "IO_x06kEvE49",
        "outputId": "5ce3840e-c641-4d49-8eab-d14cc0739c26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "IO_x06kEvE49",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "096e3f81",
      "metadata": {
        "id": "096e3f81"
      },
      "outputs": [],
      "source": [
        "# Utilities: edge channel and high-pass (noise residual) channel\n",
        "from skimage import filters, util\n",
        "\n",
        "\n",
        "def compute_edge_channel(pil_img):\n",
        "    # input PIL RGB image -> returns single-channel float32 0..1\n",
        "    gray = pil_img.convert('L')\n",
        "    arr = np.array(gray).astype(np.float32) / 255.0\n",
        "    edges = filters.sobel(arr)\n",
        "    edges = (edges - edges.min()) / (edges.max() - edges.min() + 1e-8)\n",
        "    return edges.astype(np.float32)\n",
        "\n",
        "\n",
        "def compute_noise_residual(pil_img):\n",
        "    # simple high-pass using Laplacian filter from PIL\n",
        "    gray = pil_img.convert('L')\n",
        "    lap = gray.filter(ImageFilter.FIND_EDGES)\n",
        "    arr = np.array(lap).astype(np.float32) / 255.0\n",
        "    arr = (arr - arr.mean()) / (arr.std() + 1e-8)\n",
        "    # normalize to 0..1\n",
        "    arr = (arr - arr.min()) / (arr.max() - arr.min() + 1e-8)\n",
        "    return arr.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "d9bd2ba1",
      "metadata": {
        "id": "d9bd2ba1"
      },
      "outputs": [],
      "source": [
        "# Dataset that returns 4-channel tensor + OCR text\n",
        "class DocuDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, ocr_enabled=True, tsvectorizer=None, max_tfidf_features=512):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.samples = []\n",
        "        self.transform = transform\n",
        "        self.ocr_enabled = ocr_enabled\n",
        "        # gather samples using ImageFolder-like layout\n",
        "        classes = sorted([d.name for d in self.root_dir.iterdir() if d.is_dir()])\n",
        "        class_to_idx = {c:i for i,c in enumerate(classes)}\n",
        "        for cls in classes:\n",
        "            for p in (self.root_dir/cls).glob('*'):\n",
        "                if p.suffix.lower() in ['.jpg', '.jpeg', '.png', '.tif', '.tiff', '.bmp']:\n",
        "                    self.samples.append((str(p), class_to_idx[cls]))\n",
        "        self.classes = classes\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.tsvectorizer = tsvectorizer\n",
        "\n",
        "        # If OCR + vectorizer requested but not supplied, build one lazily from all texts\n",
        "        if self.ocr_enabled and self.tsvectorizer is None:\n",
        "            print('Building OCR TF-IDF vectorizer from dataset (this may take time)...')\n",
        "            texts = []\n",
        "            for p,_ in tqdm(self.samples):\n",
        "                try:\n",
        "                    img = Image.open(p)\n",
        "                    txt = pytesseract.image_to_string(img)\n",
        "                    texts.append(txt)\n",
        "                except Exception:\n",
        "                    texts.append('')\n",
        "            self.tsvectorizer = TfidfVectorizer(max_features=max_tfidf_features)\n",
        "            self.tsvectorizer.fit(texts)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, label = self.samples[idx]\n",
        "        img = Image.open(path).convert('RGB')\n",
        "\n",
        "        # compute channels\n",
        "        edge = compute_edge_channel(img)\n",
        "        residual = compute_noise_residual(img)\n",
        "\n",
        "        # base transform for image (resize + crop handled by transform)\n",
        "        if self.transform is not None:\n",
        "            img_transformed = self.transform(img)  # returns tensor CxHxW for RGB\n",
        "        else:\n",
        "            t = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor()])\n",
        "            img_transformed = t(img)\n",
        "\n",
        "        # img_transformed is 3xHxW. convert edge/residual to tensors and concat as 4th channel\n",
        "        edge_t = torch.from_numpy(edge).unsqueeze(0)\n",
        "        resid_t = torch.from_numpy(residual).unsqueeze(0)\n",
        "\n",
        "        # Option A: use edge channel (prefer) + RGB => 4 channels where 4th is edge\n",
        "        # Here we use only one extra channel (edge). If you prefer noise residual instead, swap.\n",
        "        # We'll use edge as 4th channel; also keep residual as separate OCR-derived feature.\n",
        "        rgb = img_transformed\n",
        "        # make sure edge/resid are same HxW as rgb\n",
        "        if edge_t.shape[1:] != rgb.shape[1:]:\n",
        "            edge_t = transforms.functional.resize(edge_t, rgb.shape[1:])\n",
        "            resid_t = transforms.functional.resize(resid_t, rgb.shape[1:])\n",
        "\n",
        "        four_ch = torch.cat([rgb, edge_t], dim=0)\n",
        "\n",
        "        # OCR text -> tfidf vector\n",
        "        ocr_vec = np.zeros(self.tsvectorizer.max_features if hasattr(self.tsvectorizer, 'max_features') else 512, dtype=np.float32)\n",
        "        if self.ocr_enabled:\n",
        "            try:\n",
        "                txt = pytesseract.image_to_string(img)\n",
        "                vec = self.tsvectorizer.transform([txt]).toarray()[0].astype(np.float32)\n",
        "                ocr_vec[:vec.shape[0]] = vec\n",
        "            except Exception:\n",
        "                pass\n",
        "        ocr_vec = torch.from_numpy(ocr_vec)\n",
        "\n",
        "        return four_ch, ocr_vec, torch.tensor(label, dtype=torch.long), path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforms: training with mixup etc. Note mixup applied in training loop\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE+32, IMG_SIZE+32)),\n",
        "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.7, 1.0), ratio=(0.9,1.1), interpolation=InterpolationMode.BILINEAR),\n",
        "    transforms.RandomRotation(10, interpolation=InterpolationMode.BILINEAR),\n",
        "    transforms.ColorJitter(0.3,0.3,0.2,0.05),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "_qu0z4gThUaY"
      },
      "id": "_qu0z4gThUaY",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a lightweight simple OCR feature vector (fast) vectorizer first on training OCR texts\n",
        "print('Building simple OCR feature vector (fast) from train OCR texts...')\n",
        "train_texts = []\n",
        "for p in tqdm(glob(os.path.join(train_dir, '*', '*'))):\n",
        "    try:\n",
        "        img = Image.open(p)\n",
        "        txt = pytesseract.image_to_string(img)\n",
        "        train_texts.append(txt)\n",
        "    except Exception:\n",
        "        train_texts.append('')\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "TF_MAX_FEATURES = 256\n",
        "tfv = TfidfVectorizer(max_features=TF_MAX_FEATURES)\n",
        "if len(train_texts) > 0:\n",
        "    tfv.fit(train_texts)\n",
        "else:\n",
        "    tfv.fit([''])"
      ],
      "metadata": {
        "id": "vBd9M2ket2oB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b007b70-2dd0-4474-8c77-5e187e024eca"
      },
      "id": "vBd9M2ket2oB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building simple OCR feature vector (fast) from train OCR texts...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|‚ñé         | 85/2800 [03:26<1:39:37,  2.20s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "63c5e706",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63c5e706",
        "outputId": "a70be6b4-b6b2-4879-b87a-868d0cc0b7b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Phase1] Trainable parameters: 524801/24032833\n"
          ]
        }
      ],
      "source": [
        "train_ds = DocuDataset(train_dir, transform=train_transform, ocr_enabled=True, tsvectorizer=tfv, max_tfidf_features=TF_MAX_FEATURES)\n",
        "val_ds = DocuDataset(val_dir, transform=val_transform, ocr_enabled=True, tsvectorizer=tfv, max_tfidf_features=TF_MAX_FEATURES)\n",
        "test_ds = DocuDataset(test_dir, transform=val_transform, ocr_enabled=True, tsvectorizer=tfv, max_tfidf_features=TF_MAX_FEATURES)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "\n",
        "print('Classes:', train_ds.classes)\n",
        "print('Sizes -> Train:', len(train_ds), 'Val:', len(val_ds), 'Test:', len(test_ds))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model: ConvNeXt-B or Swin-T backbone with modified input channels\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, backbone_name='convnext_base', pretrained=True, ocr_vec_size=TF_MAX_FEATURES, hidden_dim=256, num_classes=1):\n",
        "        super().__init__()\n",
        "        self.backbone_name = backbone_name\n",
        "        # create backbone via timm, adapt for 4-channel input\n",
        "        model = timm.create_model(backbone_name, pretrained=pretrained, num_classes=0, in_chans=4)\n",
        "        self.backbone = model\n",
        "        feat_dim = self.backbone.num_features\n",
        "\n",
        "        # small projection for OCR vector\n",
        "        self.ocr_proj = nn.Sequential(\n",
        "            nn.Linear(ocr_vec_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # fusion\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(feat_dim + 128, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x4, ocr_vec):\n",
        "        # x4: Bx4xHxW\n",
        "        feats = self.backbone(x4)  # B x feat_dim\n",
        "        ocr_emb = self.ocr_proj(ocr_vec)\n",
        "        fused = torch.cat([feats, ocr_emb], dim=1)\n",
        "        out = self.classifier(fused)\n",
        "        return out.squeeze(1)"
      ],
      "metadata": {
        "id": "yf9EaCDRuAqu"
      },
      "id": "yf9EaCDRuAqu",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate model (choose 'convnext_base' or 'swin_tiny_patch4_window7_224')\n",
        "BACKBONE = 'convnext_base'  # options: 'convnext_base', 'swin_tiny_patch4_window7_224'\n",
        "model = HybridModel(backbone_name=BACKBONE, pretrained=True, ocr_vec_size=TF_MAX_FEATURES)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# Print trainable params\n",
        "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total = sum(p.numel() for p in model.parameters())\n",
        "print(f'Trainable params: {trainable}/{total}')"
      ],
      "metadata": {
        "id": "v-3XxZRhuBoq"
      },
      "id": "v-3XxZRhuBoq",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss, optimizer with label smoothing\n",
        "class SmoothBCEWithLogits(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "    def forward(self, logits, targets):\n",
        "        # targets: 0/1\n",
        "        targets = targets.float()\n",
        "        with torch.no_grad():\n",
        "            targets = targets * (1 - self.smoothing) + 0.5 * self.smoothing\n",
        "        loss = nn.functional.binary_cross_entropy_with_logits(logits, targets.unsqueeze(1))\n",
        "        return loss\n",
        "\n",
        "criterion = SmoothBCEWithLogits(smoothing=0.05)\n",
        "\n",
        "# Optimizer with differential LRs\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)"
      ],
      "metadata": {
        "id": "v2pncXTTuEEZ"
      },
      "id": "v2pncXTTuEEZ",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "fdd62d03",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdd62d03",
        "outputId": "eae91d88-f416-4101-e1a4-33a2277a92f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Phase 1: training head-only ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [01:17<00:00,  1.76s/it]\n",
            "Eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:08<00:00,  1.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head Epoch 1/5 | Train Acc=0.690 | Val Acc=0.777 | Train Loss=0.157 | Val Loss=0.139 | time=86.4s\n",
            "üèÜ Best head model updated (Val Acc=0.777)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [01:14<00:00,  1.69s/it]\n",
            "Eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:09<00:00,  1.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head Epoch 2/5 | Train Acc=0.760 | Val Acc=0.822 | Train Loss=0.136 | Val Loss=0.116 | time=83.8s\n",
            "üèÜ Best head model updated (Val Acc=0.822)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [01:15<00:00,  1.71s/it]\n",
            "Eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:10<00:00,  1.05s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head Epoch 3/5 | Train Acc=0.775 | Val Acc=0.850 | Train Loss=0.125 | Val Loss=0.106 | time=85.9s\n",
            "üèÜ Best head model updated (Val Acc=0.850)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [01:12<00:00,  1.66s/it]\n",
            "Eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:10<00:00,  1.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head Epoch 4/5 | Train Acc=0.770 | Val Acc=0.853 | Train Loss=0.123 | Val Loss=0.102 | time=83.6s\n",
            "üèÜ Best head model updated (Val Acc=0.853)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [01:15<00:00,  1.71s/it]\n",
            "Eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:09<00:00,  1.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head Epoch 5/5 | Train Acc=0.779 | Val Acc=0.848 | Train Loss=0.119 | Val Loss=0.099 | time=84.9s\n",
            "=== Phase 1 complete ===\n",
            "Best val acc so far: 0.853\n"
          ]
        }
      ],
      "source": [
        "# Mixup implementation\n",
        "def mixup_data(x, ocr, y, alpha=0.4):\n",
        "    if alpha <= 0:\n",
        "        return x, ocr, y, None, 1.0\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = x.size()[0]\n",
        "    index = torch.randperm(batch_size).to(x.device)\n",
        "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
        "    mixed_ocr = lam * ocr + (1 - lam) * ocr[index, :]\n",
        "    y_a, y_b = y, y[index]\n",
        "    return mixed_x, mixed_ocr, y_a, y_b, lam\n",
        "\n",
        "# Mixup criterion\n",
        "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
        "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training + validation loops with TTA support (for eval)\n",
        "from copy import deepcopy\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device, mixup_alpha=0.4):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for x4, ocr, labels, _ in tqdm(loader, desc='Train'):\n",
        "        x4 = x4.to(device)\n",
        "        ocr = ocr.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # mixup\n",
        "        if mixup_alpha > 0:\n",
        "            mixed_x, mixed_ocr, y_a, y_b, lam = mixup_data(x4, ocr, labels, alpha=mixup_alpha)\n",
        "            outputs = model(mixed_x, mixed_ocr)\n",
        "            loss = mixup_criterion(criterion, outputs, y_a, y_b, lam)\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).long()\n",
        "            # For accuracy estimate, use hard labels from y_a with lam majority\n",
        "            # This is approximate; final evaluation uses val set without mixup.\n",
        "            approx_preds = (torch.sigmoid(model(x4, ocr)) > 0.5).long()\n",
        "            correct += (approx_preds.cpu() == labels.cpu()).sum().item()\n",
        "            total += labels.size(0)\n",
        "        else:\n",
        "            outputs = model(x4, ocr)\n",
        "            loss = criterion(outputs, labels)\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).long()\n",
        "            correct += (preds.cpu().squeeze() == labels.cpu()).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * x4.size(0)\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    acc = correct / total if total>0 else 0\n",
        "    return avg_loss, acc\n",
        "\n",
        "# TTA inference: perform several augmentations and average probabilities\n",
        "def tta_predict(model, x4, ocr_vec, tta_transforms):\n",
        "    model.eval()\n",
        "    probs = []\n",
        "    with torch.no_grad():\n",
        "        for t in tta_transforms:\n",
        "            # t expects PIL input; but we have tensors. We'll assume transforms are simple flips\n",
        "            x_aug = x4\n",
        "            if t == 'orig':\n",
        "                x_aug = x4\n",
        "            elif t == 'hflip':\n",
        "                x_aug = torch.flip(x4, dims=[3])\n",
        "            elif t == 'vflip':\n",
        "                x_aug = torch.flip(x4, dims=[2])\n",
        "            out = model(x_aug.to(DEVICE), ocr_vec.to(DEVICE))\n",
        "            probs.append(torch.sigmoid(out).cpu().numpy())\n",
        "    probs = np.stack(probs, axis=0).mean(axis=0)\n",
        "    return probs\n",
        "\n",
        "\n",
        "def evaluate(model, loader, criterion, device, tta=True):\n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for x4, ocr, labels, _ in tqdm(loader, desc='Eval'):\n",
        "        if tta:\n",
        "            probs = tta_predict(model, x4, ocr, tta_transforms=['orig','hflip'])\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                out = model(x4.to(device), ocr.to(device))\n",
        "                probs = torch.sigmoid(out).cpu().numpy()\n",
        "        preds = (probs > 0.5).astype(int)\n",
        "        all_probs.extend(probs.tolist())\n",
        "        all_preds.extend(preds.tolist())\n",
        "        all_labels.extend(labels.numpy().tolist())\n",
        "    return np.array(all_labels), np.array(all_probs), np.array(all_preds)"
      ],
      "metadata": {
        "id": "J9caPy4-uVRH",
        "outputId": "46e696b4-c851-4886-a86b-70c703fe7f29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "J9caPy4-uVRH",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Phase 2: unfreezing layer2, layer3, layer4 and fine-tuning ===\n",
            "[Phase2] Trainable parameters: 23807489/24032833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "EPOCHS = 8\n",
        "best_auc = 0.0\n",
        "for epoch in range(EPOCHS):\n",
        "    t0 = time.time()\n",
        "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE, mixup_alpha=0.4)\n",
        "    labels_val, probs_val, preds_val = evaluate(model, val_loader, criterion, DEVICE, tta=True)\n",
        "    fpr, tpr, thr = roc_curve(labels_val, probs_val)\n",
        "    val_auc = auc(fpr, tpr)\n",
        "    print(f'Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Val AUC: {val_auc:.4f} | time: {time.time()-t0:.1f}s')\n",
        "\n",
        "    # Save best by AUC\n",
        "    if val_auc > best_auc:\n",
        "        best_auc = val_auc\n",
        "        torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'best_hybrid.pth'))\n",
        "        print('Saved best model')\n",
        "\n",
        "    # LR scheduler step based on val loss ~ use 1 - AUC as proxy\n",
        "    scheduler.step(1 - val_auc)\n",
        "\n",
        "print('Training complete. Best val AUC:', best_auc)"
      ],
      "metadata": {
        "id": "RlrJyJCrubOw"
      },
      "id": "RlrJyJCrubOw",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Threshold optimization using ROC on validation set\n",
        "best_thresh = 0.5\n",
        "best_f1 = 0.0\n",
        "from sklearn.metrics import f1_score\n",
        "labels_val, probs_val, _ = evaluate(model, val_loader, criterion, DEVICE, tta=True)\n",
        "for thresh in np.linspace(0.1, 0.9, 81):\n",
        "    preds = (probs_val > thresh).astype(int)\n",
        "    f1 = f1_score(labels_val, preds)\n",
        "    if f1 > best_f1:\n",
        "        best_f1 = f1\n",
        "        best_thresh = thresh\n",
        "print('Best threshold by F1 on val:', best_thresh, 'F1:', best_f1)"
      ],
      "metadata": {
        "id": "z5t0soAPugUZ",
        "outputId": "ff298eca-5112-4509-8855-ed391138ec02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "z5t0soAPugUZ",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [01:16<00:00,  1.75s/it]\n",
            "Eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:09<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FT Epoch 1/10 | Train Acc=0.786 | Val Acc=0.862 | Train Loss=0.119 | Val Loss=0.095 | time=86.8s\n",
            "üèÜ Best model updated (Val Acc=0.862)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [01:17<00:00,  1.76s/it]\n",
            "Eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:09<00:00,  1.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FT Epoch 2/10 | Train Acc=0.798 | Val Acc=0.868 | Train Loss=0.112 | Val Loss=0.092 | time=87.4s\n",
            "üèÜ Best model updated (Val Acc=0.868)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [01:19<00:00,  1.82s/it]\n",
            "Eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:09<00:00,  1.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FT Epoch 3/10 | Train Acc=0.800 | Val Acc=0.872 | Train Loss=0.113 | Val Loss=0.090 | time=89.2s\n",
            "üèÜ Best model updated (Val Acc=0.872)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [01:18<00:00,  1.79s/it]\n",
            "Eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:08<00:00,  1.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FT Epoch 4/10 | Train Acc=0.799 | Val Acc=0.872 | Train Loss=0.110 | Val Loss=0.090 | time=87.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [01:20<00:00,  1.83s/it]\n",
            "Eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:09<00:00,  1.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FT Epoch 5/10 | Train Acc=0.802 | Val Acc=0.873 | Train Loss=0.110 | Val Loss=0.089 | time=90.5s\n",
            "üèÜ Best model updated (Val Acc=0.873)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [01:17<00:00,  1.76s/it]\n",
            "Eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:10<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FT Epoch 6/10 | Train Acc=0.797 | Val Acc=0.878 | Train Loss=0.107 | Val Loss=0.087 | time=87.8s\n",
            "üèÜ Best model updated (Val Acc=0.878)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [01:16<00:00,  1.73s/it]\n",
            "Eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:10<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FT Epoch 7/10 | Train Acc=0.804 | Val Acc=0.878 | Train Loss=0.107 | Val Loss=0.085 | time=86.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [01:17<00:00,  1.77s/it]\n",
            "Eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:10<00:00,  1.04s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FT Epoch 8/10 | Train Acc=0.817 | Val Acc=0.882 | Train Loss=0.104 | Val Loss=0.083 | time=88.2s\n",
            "üèÜ Best model updated (Val Acc=0.882)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [01:17<00:00,  1.77s/it]\n",
            "Eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:08<00:00,  1.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FT Epoch 9/10 | Train Acc=0.816 | Val Acc=0.880 | Train Loss=0.104 | Val Loss=0.085 | time=86.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44/44 [01:21<00:00,  1.86s/it]\n",
            "Eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:10<00:00,  1.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FT Epoch 10/10 | Train Acc=0.810 | Val Acc=0.885 | Train Loss=0.103 | Val Loss=0.084 | time=92.1s\n",
            "üèÜ Best model updated (Val Acc=0.885)\n",
            "‚úÖ Two-phase training complete.\n",
            "Final best val acc: 0.885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "7b98edd3",
      "metadata": {
        "id": "7b98edd3"
      },
      "outputs": [],
      "source": [
        "# Final evaluation on test set\n",
        "model.load_state_dict(torch.load(os.path.join(SAVE_DIR, 'best_hybrid.pth'), map_location=DEVICE))\n",
        "labels_test, probs_test, preds_test = [], [], []\n",
        "labels_test, probs_test, _ = evaluate(model, test_loader, criterion, DEVICE, tta=True)\n",
        "final_preds = (probs_test > best_thresh).astype(int)\n",
        "\n",
        "print('Test classification report:')\n",
        "print(classification_report(labels_test, final_preds, target_names=train_ds.classes))\n",
        "cm = confusion_matrix(labels_test, final_preds)\n",
        "print('Confusion matrix:\\n', cm)\n",
        "\n",
        "# Save metrics\n",
        "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support\n",
        "roc_auc = roc_auc_score(labels_test, probs_test)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(labels_test, final_preds, average=None, zero_division=0)\n",
        "metrics = {\n",
        "    'roc_auc': float(roc_auc),\n",
        "    'threshold': float(best_thresh),\n",
        "    'precision_per_class': precision.tolist(),\n",
        "    'recall_per_class': recall.tolist(),\n",
        "    'f1_per_class': f1.tolist()\n",
        "}\n",
        "with open(os.path.join(SAVE_DIR, 'final_metrics.json'), 'w') as f:\n",
        "    json.dump(metrics, f, indent=2)\n",
        "print('Saved final metrics to', os.path.join(SAVE_DIR, 'final_metrics.json'))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}