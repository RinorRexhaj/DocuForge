"""
Enhanced Blur Detection Module
===============================
Specialized detection for blur/smudge tampering artifacts generated by Forgery.py
This module focuses on detecting:
- Local blur regions (Gaussian and motion blur)
- Text overlay artifacts
- Copy-paste splice boundaries
- Signature/photo paste inconsistencies
"""

import cv2
import numpy as np
from scipy import ndimage
from skimage.feature import local_binary_pattern


def detect_blur_regions(img_rgb, window_size=31, threshold_factor=1.5):
    """
    Detect locally blurred regions using Laplacian variance analysis.
    This is specifically designed to find the blur/smudge forgeries from Forgery.py.
    
    Args:
        img_rgb: Input image in RGB format
        window_size: Size of the analysis window (larger for bigger blur regions)
        threshold_factor: Multiplier for standard deviation threshold
    
    Returns:
        Blur detection heatmap normalized to [0, 1]
    """
    # Convert to grayscale (keep as uint8 for OpenCV compatibility)
    gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)
    
    # Compute Laplacian (measures sharpness - input must be uint8)
    laplacian = cv2.Laplacian(gray, cv2.CV_64F, ksize=3)
    laplacian_abs = np.abs(laplacian).astype(np.float32)
    
    # Compute local variance of Laplacian (blur measure)
    # Low variance = blurred area, high variance = sharp area
    mean_lap = cv2.blur(laplacian_abs, (window_size, window_size))
    mean_lap_sq = cv2.blur(laplacian_abs ** 2, (window_size, window_size))
    var_lap = mean_lap_sq - mean_lap ** 2
    var_lap = np.maximum(var_lap, 0)  # Ensure non-negative
    
    # Compute global statistics
    global_mean_var = np.mean(var_lap)
    global_std_var = np.std(var_lap)
    
    # Find anomalous regions (significantly lower variance = blur)
    threshold = global_mean_var - threshold_factor * global_std_var
    blur_map = (var_lap < threshold).astype(np.float32)
    
    # Enhance detection with gradient magnitude analysis
    grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
    grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
    grad_mag = np.sqrt(grad_x**2 + grad_y**2)
    
    # Compute local gradient statistics
    local_grad_mean = cv2.blur(grad_mag, (window_size, window_size))
    global_grad_mean = np.mean(grad_mag)
    
    # Low gradient magnitude indicates blur
    gradient_blur_map = (local_grad_mean < 0.6 * global_grad_mean).astype(np.float32)
    
    # Combine both approaches
    combined_blur = 0.6 * blur_map + 0.4 * gradient_blur_map
    
    # Morphological operations to clean up
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (15, 15))
    combined_blur = cv2.morphologyEx(combined_blur, cv2.MORPH_CLOSE, kernel)
    
    # Apply Gaussian smoothing for better visualization
    combined_blur = cv2.GaussianBlur(combined_blur, (21, 21), 0)
    
    # Normalize to [0, 1]
    if combined_blur.max() > 0:
        combined_blur = combined_blur / combined_blur.max()
    
    return combined_blur


def detect_motion_blur(img_rgb, num_angles=16):
    """
    Detect motion blur artifacts by analyzing directional blur patterns.
    
    Args:
        img_rgb: Input image in RGB format
        num_angles: Number of angles to test for motion blur
    
    Returns:
        Motion blur detection heatmap normalized to [0, 1]
    """
    gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY).astype(np.float32)
    h, w = gray.shape
    
    # Compute FFT
    f_transform = np.fft.fft2(gray)
    f_shift = np.fft.fftshift(f_transform)
    magnitude_spectrum = np.log(np.abs(f_shift) + 1)
    
    # Analyze directional energy in frequency domain
    center_y, center_x = h // 2, w // 2
    motion_scores = np.zeros((h, w), dtype=np.float32)
    
    for angle_idx in range(num_angles):
        angle = np.pi * angle_idx / num_angles
        
        # Create directional filter
        y, x = np.ogrid[:h, :w]
        y_centered = y - center_y
        x_centered = x - center_x
        
        # Line passing through center at this angle
        line_dist = np.abs(y_centered * np.cos(angle) - x_centered * np.sin(angle))
        directional_mask = (line_dist < 3).astype(np.float32)
        
        # Measure energy along this direction
        directional_energy = np.sum(magnitude_spectrum * directional_mask)
        
        # Inverse FFT with this directional component
        filtered = f_shift * directional_mask
        filtered_shift = np.fft.ifftshift(filtered)
        img_back = np.fft.ifft2(filtered_shift)
        img_back = np.abs(img_back)
        
        # Accumulate motion blur evidence
        motion_scores += img_back * directional_energy / (num_angles * 1e6)
    
    # Normalize
    if motion_scores.max() > 0:
        motion_scores = motion_scores / motion_scores.max()
    
    # Smooth and threshold
    motion_scores = cv2.GaussianBlur(motion_scores, (15, 15), 0)
    
    return motion_scores


def detect_text_overlay_artifacts(img_rgb):
    """
    Detect text overlay artifacts (white rectangles with text).
    
    Args:
        img_rgb: Input image in RGB format
    
    Returns:
        Text overlay detection heatmap normalized to [0, 1]
    """
    # Convert to grayscale and HSV
    gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)
    hsv = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)
    
    # Detect bright regions (white overlays)
    _, bright_mask = cv2.threshold(gray, 240, 255, cv2.THRESH_BINARY)
    
    # Detect low saturation (white/gray areas)
    saturation = hsv[:, :, 1]
    _, low_sat = cv2.threshold(saturation, 30, 255, cv2.THRESH_BINARY_INV)
    
    # Combine bright and low saturation
    overlay_candidates = cv2.bitwise_and(bright_mask, low_sat)
    
    # Find rectangular regions (text overlays are often rectangular)
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (20, 5))
    overlay_candidates = cv2.morphologyEx(overlay_candidates, cv2.MORPH_CLOSE, kernel)
    
    # Detect edges (text has strong edges)
    edges = cv2.Canny(gray, 50, 150)
    
    # Find regions with many edges (text-like)
    edge_density = cv2.blur(edges.astype(np.float32), (15, 15))
    
    # Combine overlay candidates with edge density
    text_overlay_map = (overlay_candidates.astype(np.float32) / 255.0) * 0.5
    text_overlay_map += (edge_density / 255.0) * 0.5
    
    # Detect color inconsistencies (edited text is often red/different color)
    b, g, r = cv2.split(img_rgb)
    red_emphasis = (r.astype(np.float32) - 0.5 * g.astype(np.float32) - 0.5 * b.astype(np.float32))
    red_emphasis = np.maximum(red_emphasis, 0)
    red_emphasis = (red_emphasis / (red_emphasis.max() + 1e-8)) * 0.3
    
    text_overlay_map += red_emphasis
    
    # Normalize
    text_overlay_map = np.clip(text_overlay_map, 0, 1)
    text_overlay_map = cv2.GaussianBlur(text_overlay_map, (11, 11), 0)
    
    return text_overlay_map


def detect_splice_boundaries(img_rgb, edge_threshold=100):
    """
    Detect copy-paste splice boundaries using edge analysis and texture discontinuities.
    
    Args:
        img_rgb: Input image in RGB format
        edge_threshold: Threshold for edge detection
    
    Returns:
        Splice boundary detection heatmap normalized to [0, 1]
    """
    gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)
    
    # Multi-scale edge detection
    edges1 = cv2.Canny(gray, 50, 150)
    edges2 = cv2.Canny(gray, 30, 100)
    
    # Detect sudden edge transitions (splice boundaries)
    kernel_dilate = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))
    edges_dilated = cv2.dilate(edges1, kernel_dilate, iterations=1)
    
    # Find closed contours (pasted regions have clear boundaries)
    contours, _ = cv2.findContours(edges_dilated, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    boundary_map = np.zeros_like(gray, dtype=np.float32)
    
    for contour in contours:
        area = cv2.contourArea(contour)
        if 500 < area < img_rgb.shape[0] * img_rgb.shape[1] * 0.3:  # Reasonable patch size
            # Draw thick boundary
            cv2.drawContours(boundary_map, [contour], -1, 1.0, thickness=10)
    
    # Add edge strength
    boundary_map += edges1.astype(np.float32) / 255.0 * 0.3
    boundary_map += edges2.astype(np.float32) / 255.0 * 0.2
    
    # Texture discontinuity using LBP
    try:
        lbp = local_binary_pattern(gray, P=8, R=1, method='uniform')
        lbp_var = cv2.blur(lbp, (15, 15))
        lbp_grad = np.abs(cv2.Laplacian(lbp_var, cv2.CV_64F))
        lbp_grad = (lbp_grad - lbp_grad.min()) / (lbp_grad.max() - lbp_grad.min() + 1e-8)
        boundary_map += lbp_grad * 0.3
    except Exception:
        pass
    
    # Smooth and normalize
    boundary_map = cv2.GaussianBlur(boundary_map, (9, 9), 0)
    boundary_map = np.clip(boundary_map, 0, 1)
    
    return boundary_map


def detect_illumination_inconsistency(img_rgb):
    """
    Detect illumination inconsistencies from pasted regions.
    
    Args:
        img_rgb: Input image in RGB format
    
    Returns:
        Illumination inconsistency heatmap normalized to [0, 1]
    """
    # Convert to LAB color space
    lab = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2LAB)
    l_channel = lab[:, :, 0].astype(np.float32)
    
    # Compute local mean illumination
    window_size = 31
    local_mean = cv2.blur(l_channel, (window_size, window_size))
    
    # Compute expected illumination (global trend)
    large_window = 101
    global_trend = cv2.blur(l_channel, (large_window, large_window))
    
    # Find deviations
    deviation = np.abs(local_mean - global_trend)
    
    # Normalize
    deviation = (deviation - deviation.min()) / (deviation.max() - deviation.min() + 1e-8)
    
    # Threshold and enhance
    illumination_map = np.power(deviation, 0.8)  # Enhance contrast
    illumination_map = cv2.GaussianBlur(illumination_map, (15, 15), 0)
    
    return illumination_map


def comprehensive_forgery_detection(img_rgb):
    """
    Comprehensive detection combining all specialized techniques.
    
    Args:
        img_rgb: Input image in RGB format
    
    Returns:
        Dictionary with individual detection maps and combined result
    """
    print("  ðŸ” Detecting blur regions...")
    blur_map = detect_blur_regions(img_rgb, window_size=31, threshold_factor=1.5)
    
    print("  ðŸ” Detecting motion blur...")
    motion_blur_map = detect_motion_blur(img_rgb, num_angles=16)
    
    print("  ðŸ” Detecting text overlays...")
    text_map = detect_text_overlay_artifacts(img_rgb)
    
    print("  ðŸ” Detecting splice boundaries...")
    splice_map = detect_splice_boundaries(img_rgb)
    
    print("  ðŸ” Detecting illumination inconsistencies...")
    illumination_map = detect_illumination_inconsistency(img_rgb)
    
    # Weighted combination - emphasize blur detection for your forgeries
    combined = (
        0.25 * blur_map +           # Heavy weight on blur
        0.20 * motion_blur_map +    # Motion blur detection
        0.20 * text_map +           # Text overlay detection
        0.20 * splice_map +         # Copy-paste boundaries
        0.15 * illumination_map     # Illumination issues
    )
    
    # Normalize
    combined = (combined - combined.min()) / (combined.max() - combined.min() + 1e-8)
    
    return {
        'blur': blur_map,
        'motion_blur': motion_blur_map,
        'text_overlay': text_map,
        'splice_boundary': splice_map,
        'illumination': illumination_map,
        'combined': combined
    }
